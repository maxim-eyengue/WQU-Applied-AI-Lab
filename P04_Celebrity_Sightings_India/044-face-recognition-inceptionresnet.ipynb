{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">‚úó</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">‚úó</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Inception-ResNet V1 for Facial Recognition  \n",
    "\n",
    "### üìå Summary  \n",
    "In this lesson, we'll explore **Inception-ResNet V1**, a powerful deep learning model for **facial recognition**. Our goal is to build a system capable of recognizing the face of **Mary Kom**, a legendary Indian boxer ü•ä.  \n",
    "\n",
    "### üéØ Objectives  \n",
    "- üîç **Detect faces** using **MTCNN** (Multi-task Cascaded Convolutional Networks)  \n",
    "- üß† **Generate face embeddings** with **Inception-ResNet V1**  \n",
    "- üñºÔ∏è **Recognize faces** in images using the generated embeddings  \n",
    "\n",
    "### üÜï New Terms  \n",
    "- **Inception-ResNet V1** üèóÔ∏è: A deep learning model that combines the **Inception** architecture with **ResNet (Residual Networks)** to improve efficiency and accuracy in image processing.  \n",
    "- **Facial Recognition** üè∑Ô∏è: A technology that identifies or verifies a person's identity using facial features extracted from images or videos.  \n",
    "- **Face Embedding** üî¢: A numerical representation of a face generated by a neural network, which allows the model to compare and recognize faces more effectively.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get ready for this lesson by importing the packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary import\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll print out the version numbers for our libraries, including Python. We want to make sure that anyone who reviews our work knows exactly what software we used in case they want to reproduce our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: linux\n",
      "Python version: 3.11.0 (main, Nov 15 2022, 20:12:54) [GCC 10.2.1 20210110]\n",
      "---\n",
      "PIL version :  10.2.0\n",
      "torch version :  2.2.2+cu121\n",
      "torchvision version :  0.17.2+cu121\n"
     ]
    }
   ],
   "source": [
    "# Libraries and system versions\n",
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"PIL version : \", PIL.__version__)\n",
    "print(\"torch version : \", torch.__version__)\n",
    "print(\"torchvision version : \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've done in past lessons, we'll also check if GPUs are available. Remember that some computers come with GPUs, which allow for bigger and faster model training. The `cuda` package is used to access GPUs on Linux and Windows machines in PyTorch; `mps` is used on Macs. \n",
    "\n",
    "We'll use the `device` variable later to set the location of our data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "# Check the GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Print the GPU device\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing MTCNN and Inception-ResNet V1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start things by initializing MTCNN as we've done in the past lesson. We'll want an MTCNN model that just detects one face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.1:** Create a MTCNN model. Make sure to set the image size as 240, `keep_all` to `False`, and`min_face_size` to 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn0 = ...\n",
    "print(mtcnn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also initialize the Inception-ResNet V1 model. This model is used for facial recognition. Particularly, we'll be using a model that has been pre-trained on the VGGFace2 dataset. This is a massive dataset of over 3 million images and over 9000 identities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained=\"vggface2\").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare our data to make it easy to use the Inception-ResNet V1 model. We'll ultimately create a `DataLoader` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.2:** Create a path object for the path `project4/data/images/`. Use the `Path` class from `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = ...\n",
    "\n",
    "print(f\"Path to images: {images_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an `ImageFolder` object for the path we just created. `ImageFolder` is provided to us by `torchvision` and makes it easier when working with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.3:** Create an `ImageFolder` object for the path object in the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ...\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `ImageFolder` object, each subdirectory of the input path is considered a separate class. The class label is just the name of the subdirectory. In the previous lesson, we pulled out frames for Mary Kom and Ranveer, the interviewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.4:** Print out all subdirectories in `images_folder`. You should use the method `iterdir` of the path object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdirectory in ...\n",
    "    print(subdirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `ImageFolder` object, the `.class_to_idx` is a mapping between class label to class integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we'd like to create the reverse mapping. In other words, integer to class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.5:** Create a dictionary that maps integer to class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = ...\n",
    "\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a `DataLoader` object with our `ImageFolder`. These objects are iterables that work well with PyTorch. Remember how we worked with `DataLoader` in past projects. One thing we'll do differently here is provide the `DataLoader` a collate function. In our case, the collate function is returning the first element of a tuple, which is the image object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.6:** Construct a `DataLoader` object with the previously created `dataset`. Make sure to define the collate function defined above with the keyword argument `collate_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ...\n",
    "\n",
    "print(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to start using our facial recognition model. We have a `DataLoader` object that we can iterate over all the images when using the model but let's first see how it's done with just a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = iter(loader).__next__()\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to our facial recognition model is to first detect the faces in the image. We can use MTCNN model like in the previous lesson. From `img`, we only see one person, Mary Kom. So it's fine to use `mtcnn0`, which will only return one detected face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.7:** Use `mtcnn0` to detect the face and probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face, prob = ...\n",
    "\n",
    "print(type(face))\n",
    "print(f\"Probability of detected face: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are really certain we have detected a face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Inception-ResNet V1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The facial recognition model needs a 4D PyTorch tensor of faces. At the moment, we have just one 3D tensor because we only have one face. We can't use the model right away! Let's see what happens if we try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resnet(face)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.8:** Print out the shape of our `face` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it's not a 4D tensor! We'll need to change the shape of the Tensor for us to make progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.9:** Change the shape of the tensor from `[3, 240, 240]` to `[1, 3, 240, 240]`. You'll need to use the `.unsqueeze` method of the PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_4d = ...\n",
    "\n",
    "print(face_4d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Inception-ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = ...\n",
    "\n",
    "print(f\"Shape of face embedding: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns an embedding for the face using 512 dimensions. The embedding is a vector that represents the features the model extracted. Now we are ready to run our model on all the extracted frames. The process will be:\n",
    "\n",
    "- Iterate over all images\n",
    "- Run the face detection model\n",
    "- If the model returns a result and with a probability of at least 90%, add the embeddings to a dictionary that keeps the embeddings for each person separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.10:** Filter out results in which `face` is `None` or where the probability is less than 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps name to list of their embeddings\n",
    "name_to_embeddings = {name: [] for name in idx_to_class.values()}\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn0(img, return_prob=True)\n",
    "    if ...:\n",
    "        emb = resnet(face.unsqueeze(0))\n",
    "        name_to_embeddings[idx_to_class[idx]].append(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we had several images of Mary Kom and Ranveer, we have several face embeddings for each of them. We'll want to create a single face embedding for each person by taking the average across their embeddings. This average face embedding will be our reference for a person, often called a faceprint in analogy with fingerprints.\n",
    "\n",
    "The first step is to take our list of face embeddings for a person and create one 2D PyTorch tensor. This can be done with `torch.stack` given our `name_to_embedings` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_to_embeddings.keys())\n",
    "print(type(name_to_embeddings[\"mary_kom\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.11:** Take the list of embeddings for both Mary and Ranveer and convert each list to a 2D PyTorch tensor by using `torch.stack`. Then print the shapes of the two PyTorch tensors resulting from the stacking operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_mary = ...\n",
    "embeddings_ranveer = ...\n",
    "\n",
    "embeddings_mary_shape = ...\n",
    "embeddings_ranveer_shape = ...\n",
    "\n",
    "print(f\"Shape of stack of embeddings for Mary: {embeddings_mary_shape}\")\n",
    "print(f\"Shape of stack of embeddings for Ranveer: {embeddings_ranveer_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the average using `torch.mean`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.12:** Calculate the average of the stacked embeddings along the zero dimension. Make sure to do this for the embeddings for both Mary Kom and Ranveer. The resulting shape should be a 1x512 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embedding_mary = ...\n",
    "avg_embedding_ranveer = ...\n",
    "\n",
    "print(f\"Shape of avg_embedding: {avg_embedding_mary.shape}\")\n",
    "print(f\"Shape of avg_embedding: {avg_embedding_ranveer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the embeddings as a list of tuples. The tuples will be in the form of `(average embedding, name)`. Since we have two different people, our list will have length two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.13:** Create the list to save the embedding. Use the string `\"mary_kom\"` and `\"ranveer\"` for Mary Kom and Ranveer, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_save = ...\n",
    "\n",
    "torch.save(embeddings_to_save, \"embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code cell, we saved the average embeddings to file. If we need the embedding, we can load it up with `torch.load`. It accepts as its input the path to the saved embedding. We saved it to the current directory with the name `\"embeddings.pt\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.14:** Load the face embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data = ...\n",
    "\n",
    "names = [name for _, name in embedding_data]\n",
    "print(f\"Loaded the embedding for: {names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception-ResNet V1 on an Image with One Person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider the embeddings of Mary Kom and Ranveer as our database of known faces. If we now take a new image, we can check if we can recognize whether Mary Kom or Ranveer appear in that image! We'll first focus on one image with only Mary Kom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.15:** Create a path object for the image in `project4/data/extracted_frames/frame_100.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = ...\n",
    "\n",
    "test_img = Image.open(test_img_path)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we had used `mtcnn0` to just detect one face but let's practice with one that will detect all faces in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size=240, keep_all=True, min_face_size=40)\n",
    "print(f\"MTCNN image size: {mtcnn.image_size}\")\n",
    "print(f\"MTCNN keeping all faces: {mtcnn.keep_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.16:** Run the face detection model `mtcnn` on the selected frame from above. Recall how `mtcnn` was set to find all faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cropped_list, prob_list = ...\n",
    "\n",
    "print(f\"Number of detected faces: {len(prob_list)}\")\n",
    "print(f\"Probability of detected face: {prob_list[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will then run the face recognition model to get the face embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prob in enumerate(prob_list):\n",
    "    if prob > 0.90:\n",
    "        emb = resnet(img_cropped_list[i].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the embedding we created above, `emb`, we can see how different it's from the average embedding for Mary Kom. Let's compute the distance between `emb` and the two saved embeddings of Mary and Ranveer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.17:** Use `torch.dist` to calculate the distance between `emb` and `known_emb`. Make sure that `dist` is a float and not a PyTorch tensor. The calculated distances will be stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = {}\n",
    "\n",
    "for known_emb, name in embedding_data:\n",
    "    dist = ...\n",
    "    distances[name] = dist\n",
    "\n",
    "closest, min_dist = min(distances.items(), key=lambda x: x[1])\n",
    "print(f\"Closest match: {closest}\")\n",
    "print(f\"Calculated distance: {min_dist :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest distance corresponds to Mary Kom. It makes sense the distance is closer for Mary Kom as that's who is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `mtcnn.detect` to get the bounding boxes for the faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, _ = mtcnn.detect(test_img)\n",
    "print(f\"Shape of boxes tensor: {boxes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's draw a box over a face together with the name of its closest known embedding. We'll use a threshold of 0.8. Any detected face with an embedding that is further than this threshold from a known face will remain unrecognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.18:** Recognize Mary Kom's face by drawing a box with her name next to it. Only do so for faces where `min_dist` is smaller than the `threshold`. The code below displays the original image and plots the bounding boxes for the detected faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets the image size and draws the original image\n",
    "width, height = test_img.size\n",
    "dpi = 96\n",
    "fig = plt.figure(figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "axis = fig.subplots()\n",
    "axis.imshow(test_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "for box in boxes:\n",
    "    rect = plt.Rectangle(\n",
    "        (box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, color=\"blue\"\n",
    "    )\n",
    "    axis.add_patch(rect)\n",
    "\n",
    "    closest, min_dist = min(distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Drawing the box with recognition results\n",
    "\n",
    "    if ...:\n",
    "        name = closest\n",
    "        color = \"blue\"\n",
    "    else:\n",
    "        name = \"Unrecognized\"\n",
    "        color = \"red\"\n",
    "\n",
    "    plt.text(\n",
    "        box[0],\n",
    "        box[1],\n",
    "        f\"{name} {min_dist:.2f}\",\n",
    "        fontsize=12,\n",
    "        color=color,\n",
    "        ha=\"left\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we were able to recognize Mary Kom given our specified threshold!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception-ResNet V1 on an Image With More Than One Person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run our facial recognition model when there are more than one person in the image. We have chosen the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.19:** Display the chosen image `project4/data/extracted_frames/frame_210.jpg` using `Image.open`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_multiple_people_path = Path(\"project4\", \"data\", \"extracted_frames\", \"frame_210.jpg\")\n",
    "img_multiple_people = ...\n",
    "\n",
    "img_multiple_people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's six faces in that image, seven if you count the panda! The function below encapsulates the code we have already written. It accepts the path of an image and it will draw boxes over each detected face and indicate where it was able to recognize any of the two faces from our embeddings we saved earlier. If any face is recognized, the person's name is displayed along with the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces(img_path, embedding_data, mtcnn, resnet, threshold=0.7):\n",
    "    # Generating the bounding boxes, faces tensors, and probabilities\n",
    "    image = Image.open(img_path)\n",
    "    boxes, probs = mtcnn.detect(image)\n",
    "    cropped_images = mtcnn(image)\n",
    "\n",
    "    if boxes is None:\n",
    "        return\n",
    "\n",
    "    # This sets the image size and draws the original image\n",
    "    width, height = image.size\n",
    "    dpi = 96\n",
    "    fig = plt.figure(figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    axis = fig.subplots()\n",
    "    axis.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Iterating over each face and comparing it against the pre-calculated embeddings\n",
    "    # from our \"database\"\n",
    "    for box, prob, face in zip(boxes, probs, cropped_images):\n",
    "        if prob < 0.90:\n",
    "            continue\n",
    "\n",
    "        # Draw bounding boxes for all detected faces\n",
    "        rect = plt.Rectangle(\n",
    "            (box[0], box[1]),\n",
    "            box[2] - box[0],\n",
    "            box[3] - box[1],\n",
    "            fill=False,\n",
    "            color=\"blue\",\n",
    "        )\n",
    "        axis.add_patch(rect)\n",
    "\n",
    "        # Find the closest face from our database of faces\n",
    "        emb = resnet(face.unsqueeze(0))\n",
    "        distances = {}\n",
    "        for known_emb, name in embedding_data:\n",
    "            dist = torch.dist(emb, known_emb).item()\n",
    "            distances[name] = dist\n",
    "\n",
    "        closest, min_dist = min(distances.items(), key=lambda x: x[1])\n",
    "\n",
    "        # Drawing the box with recognition results\n",
    "        name = closest if min_dist < threshold else \"Unrecognized\"\n",
    "        color = \"red\" if name == \"Unrecognized\" else \"blue\"\n",
    "        label = f\"{name} {min_dist:.2f}\"\n",
    "\n",
    "        axis.text(box[0], box[1], label, fontsize=8, color=color)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function defined, it's time to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_faces(img_multiple_people_path, embedding_data, mtcnn, resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model failed to recognize anyone. Keep in mind this video frame is showing a photo of Mary Kom's family. It might be having difficulty for this reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4.20:** Run `recognize_faces` but use a higher threshold of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_faces(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we recognize Mary Kom. However, we have a false positive as the model thinks Mary Kom's daughter is Ranveer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Conclusion  \n",
    "\n",
    "In this lesson, we explored how to use **Inception-ResNet V1** for facial recognition. Here are the key takeaways:  \n",
    "\n",
    "üìå **Inception-ResNet V1** is a deep learning model designed for facial recognition.  \n",
    "üìå It processes **extracted faces** from an image as input.  \n",
    "üìå **MTCNN** (Multi-task Cascaded Convolutional Networks) helps detect and extract faces from images.  \n",
    "üìå **Inception-ResNet V1** generates **face embeddings**, which are numerical representations of facial features.  \n",
    "üìå We can compare face embeddings by calculating the **distance** between them.  \n",
    "üìå **Smaller distances** between two embeddings suggest they likely belong to the **same person**.  \n",
    "\n",
    "üöÄ Now that we understand how to recognize faces, we can take it a step further by building a **web app** that integrates **face detection and recognition**! üé≠üíª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
