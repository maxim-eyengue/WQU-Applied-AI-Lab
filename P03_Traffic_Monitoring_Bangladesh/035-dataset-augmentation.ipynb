{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">‚úó</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">‚úó</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Enhancing Object Detection with Data Augmentation üñºÔ∏è\n",
    "\n",
    "### Summary:  \n",
    "This session focuses on **data augmentation** for object detection tasks. üïµÔ∏è‚Äç‚ôÇÔ∏è We begin by examining the data used to train a YOLO model, showcasing how augmented data is derived from original images. The session then demonstrates using **Torchvision transforms** to manually generate similar augmented data, while emphasizing the importance of transforming bounding boxes along with images. üñåÔ∏è  \n",
    "\n",
    "\n",
    "#### ü•Ö Objectives:\n",
    "1. **üîç Explore Training Data in YOLO**: Understand how data is loaded and prepared during YOLO model training.  \n",
    "2. **üìä Observe YOLO's Augmented Training Data**: Analyze examples of augmented data used by YOLO to enhance model performance.  \n",
    "3. **üõ†Ô∏è Use Torchvision Transforms**: Apply **Torchvision** augmentation techniques to manipulate image data.  \n",
    "4. **üéØ Handle Bounding Boxes**: Learn how to appropriately transform bounding boxes to align with augmented images.  \n",
    "5. **‚öôÔ∏è Compose Augmentation Techniques**: Combine multiple augmentation methods for a robust data preparation pipeline.  \n",
    "\n",
    "#### üí° New Term:\n",
    "- **Data Augmentation**: A technique used to artificially increase the size and diversity of a dataset by applying transformations (e.g., rotation, flipping, scaling) to existing images, enhancing model generalization. üöÄ\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the packages we'll need in this notebook.  Most are familiar.  We will need version 2 of the `torchvision.transforms` module here.  The API is slightly different than that of version 1 that we've used previously, but it's pretty similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Necessary import\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchinfo\n",
    "import torchvision\n",
    "import ultralytics\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to reproduce this notebook in the future, we'll record the version information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: linux\n",
      "Python version: 3.11.0 (main, Nov 15 2022, 20:12:54) [GCC 10.2.1 20210110]\n",
      "---\n",
      "matplotlib version: 3.9.2\n",
      "PIL version: 10.2.0\n",
      "torch version: 2.2.2+cu121\n",
      "torchvision version: 0.17.2+cu121\n",
      "ultralytics version: 8.3.27\n"
     ]
    }
   ],
   "source": [
    "# Libraries and system versions\n",
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"matplotlib version:\", plt.matplotlib.__version__)\n",
    "print(\"PIL version:\", Image.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)\n",
    "print(\"ultralytics version:\", ultralytics.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the classes of the Dhaka AI data set we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS_DICT type, <class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'ambulance',\n",
       " 1: 'army vehicle',\n",
       " 2: 'auto rickshaw',\n",
       " 3: 'bicycle',\n",
       " 4: 'bus',\n",
       " 5: 'car',\n",
       " 6: 'garbagevan',\n",
       " 7: 'human hauler',\n",
       " 8: 'minibus',\n",
       " 9: 'minivan',\n",
       " 10: 'motorbike',\n",
       " 11: 'pickup',\n",
       " 12: 'policecar',\n",
       " 13: 'rickshaw',\n",
       " 14: 'scooter',\n",
       " 15: 'suv',\n",
       " 16: 'taxi',\n",
       " 17: 'three wheelers (CNG)',\n",
       " 18: 'truck',\n",
       " 19: 'van',\n",
       " 20: 'wheelbarrow'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crete class dictionary mapping classes indexes with classes names\n",
    "CLASS_DICT = dict(\n",
    "    enumerate(\n",
    "        [\n",
    "            \"ambulance\",\n",
    "            \"army vehicle\",\n",
    "            \"auto rickshaw\",\n",
    "            \"bicycle\",\n",
    "            \"bus\",\n",
    "            \"car\",\n",
    "            \"garbagevan\",\n",
    "            \"human hauler\",\n",
    "            \"minibus\",\n",
    "            \"minivan\",\n",
    "            \"motorbike\",\n",
    "            \"pickup\",\n",
    "            \"policecar\",\n",
    "            \"rickshaw\",\n",
    "            \"scooter\",\n",
    "            \"suv\",\n",
    "            \"taxi\",\n",
    "            \"three wheelers (CNG)\",\n",
    "            \"truck\",\n",
    "            \"van\",\n",
    "            \"wheelbarrow\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"CLASS_DICT type,\", type(CLASS_DICT))\n",
    "CLASS_DICT # displaying the class dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation in Our YOLO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we passed our training images to the YOLO model and let it do its thing.  The obvious assumption to make is that these images would be used as is, but it turns out not to be so.  To demonstrate what was happening, we'll load the model back up and poke around inside of it a bit.\n",
    "\n",
    "Let's start by finding a saved version of the model.  This cell should show all of the training runs that have been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('runs/detect/train')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model information path directory\n",
    "runs_dir = pathlib.Path(\"runs\", \"detect\")\n",
    "# List training folders\n",
    "list(runs_dir.glob(\"train*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "If you don't see anything listed here, go back and run the previous notebook all the way through!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.1:** Choose a training run, and check that there are model weights saved in the `weights/best.pt` file for that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights file exists? True\n"
     ]
    }
   ],
   "source": [
    "# Set the train directory path\n",
    "run_dir = runs_dir / \"train\"\n",
    "# Path to file with the best weights during model's training\n",
    "weights_file = run_dir / \"weights\" / \"best.pt\"\n",
    "\n",
    "print(\"Weights file exists?\", weights_file.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.2:** Load the model from the weights file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "YOLO                                                    --\n",
       "‚îú‚îÄDetectionModel: 1-1                                   --\n",
       "‚îÇ    ‚îî‚îÄSequential: 2-1                                  --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-1                                   (464)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-2                                   (4,672)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-3                                    (7,360)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-4                                   (18,560)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-5                                    (49,664)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-6                                   (73,984)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-7                                    (197,632)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-8                                   (295,424)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-9                                    (460,288)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSPPF: 3-10                                  (164,608)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄUpsample: 3-11                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-12                                --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-13                                   (148,224)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄUpsample: 3-14                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-15                                --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-16                                   (37,248)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-17                                  (36,992)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-18                                --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-19                                   (123,648)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-20                                  (147,712)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-21                                --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄC2f: 3-22                                   (493,056)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDetect: 3-23                                (755,407)\n",
       "================================================================================\n",
       "Total params: 3,014,943\n",
       "Trainable params: 0\n",
       "Non-trainable params: 3,014,943\n",
       "================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = YOLO(model = weights_file)\n",
    "\n",
    "# Summary info about the model\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the model set up to load the data.  The easiest way to do that is to train it for an epoch.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "When you call <code>.train()</code> on a YOLO model, it sets up a data loader, if it doesn't already exist.  Unfortunately, there's no easy way to trigger that set-up step without doing an epoch of training. üòî\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.3:** Run one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.27 üöÄ Python-3.11.0 torch-2.2.2+cu121 CUDA:0 (Tesla T4, 14918MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/train/weights/best.pt, data=data.yaml, epochs=1, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=1, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755407  ultralytics.nn.modules.head.Detect           [21, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3,014,943 parameters, 3,014,927 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /app/data_yolo/labels/train.cache... 2422 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2422/2422 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /app/data_yolo/labels/val.cache... 578 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 578/578 [00:00<?, ?it/s]\n",
      "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0004, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 1 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.59G      1.168      1.218      1.048         95        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 303/303 [01:28<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        578       4590      0.613      0.363      0.414      0.286\n",
      "\n",
      "1 epochs completed in 0.030 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.27 üöÄ Python-3.11.0 torch-2.2.2+cu121 CUDA:0 (Tesla T4, 14918MiB)\n",
      "Model summary (fused): 168 layers, 3,009,743 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:12<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        578       4590       0.61      0.366      0.414      0.286\n",
      "             ambulance         17         17          1          0    0.00737    0.00601\n",
      "          army vehicle         10         12          1      0.164      0.427      0.359\n",
      "         auto rickshaw         37        111      0.557      0.486      0.523      0.322\n",
      "               bicycle         72         93      0.408       0.28      0.326      0.155\n",
      "                   bus        295        645      0.679       0.69       0.73      0.515\n",
      "                   car        309       1006      0.748      0.677       0.74      0.517\n",
      "          human hauler         23         25      0.368       0.24      0.201      0.145\n",
      "               minibus         10         11          0          0     0.0416     0.0286\n",
      "               minivan        128        208      0.387      0.442      0.352      0.255\n",
      "             motorbike        238        449      0.647      0.561      0.601      0.345\n",
      "                pickup        157        217      0.743      0.429      0.558       0.38\n",
      "             policecar          6          6          1          0      0.223      0.166\n",
      "              rickshaw        202        653      0.475      0.749      0.706      0.457\n",
      "               scooter          8          9          1          0    0.00812     0.0049\n",
      "                   suv        105        161      0.274      0.514      0.366      0.271\n",
      "                  taxi          8         10          1          0       0.55      0.435\n",
      "  three wheelers (CNG)        224        510      0.686      0.658      0.697      0.483\n",
      "                 truck        159        289      0.558       0.73      0.694      0.502\n",
      "                   van         86        139      0.302      0.432      0.246      0.164\n",
      "           wheelbarrow         16         19      0.359      0.263      0.279      0.203\n",
      "Speed: 0.2ms preprocess, 2.7ms inference, 0.0ms loss, 3.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# YOLO model training\n",
    "result = model.train(\n",
    "    data = model.overrides[\"data\"],\n",
    "    epochs = 1,\n",
    "    batch = 8,\n",
    "    workers = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should now have a `.trainer` attribute, which has a `.train_loader` attribute.  This will be a `DataLoader` that loads the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.4:** Save this data loader to variable `loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.data.build.InfiniteDataLoader'>\n"
     ]
    }
   ],
   "source": [
    "# Get the data loader of the model\n",
    "loader = model.trainer.train_loader\n",
    "\n",
    "print(type(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loaders are _iterables_.  That is, you can put them in a `for` loop to load data one batch at a time.  We just want to read one batch from it, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.5:** Load one batch from `loader` into the variable `batch`.  You can do this by constructing a `for` loop over `loader` and calling `break` inside the loop, so that it only runs once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Load one batch from the YOLO model data loader\n",
    "for batch in loader:\n",
    "    break # stop after getting the first element of the data loader\n",
    "    \n",
    "print(type(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "A more advanced way to accomplish this same thing is: <code>batch = next(iter(loader))</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a dictionary. (What a surprise!)  Let's explore what's in this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.6:** Print out the keys in `batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['im_file', 'ori_shape', 'resized_shape', 'img', 'cls', 'bboxes', 'batch_idx'])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys()) # printing the keys of the YOLO model's first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.7:** Print out the shape of the `img` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"img\"].shape) # batch images shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of 3 represents the color channels.  The dimension of 640 are the width and height.  So what does the dimension of 8 represent?\n",
    "\n",
    "You can get a clue by reviewing the call to `model.train`.  We set a batch size of 8.  This tensor thus represents eight training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.8:** Print out the shape of the `bboxes` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102, 4])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"bboxes\"].shape) # bounding boxes shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a lot of bounding boxes for one image, so these must be the boxes for all of the images in the batch.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "The exact number of bounding boxes will depend on the random batch that got delivered to you.  If you re-run the cell that creates <code>batch</code>, you'll find that you get another number here.\n",
    "</div>\n",
    "\n",
    "The image index in the batch that the box corresponds to is given in the `batch_idx` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 6., 6., 6., 6., 7., 7., 7., 7., 7., 7., 7.,\n",
      "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"batch_idx\"]) # indexes matching images and bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can select the bounding boxes for a particular image in a batch by finding the rows that correspond to a particular batch index value.  This is implemented for us in the following function, which will plot the bounding boxes on top of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot images with their bounding boxes\n",
    "def plot_with_bboxes(img, bboxes, cls, batch_idx = None, index = 0, **kw):\n",
    "    \"\"\"Plot the bounding boxes on an image.\n",
    "\n",
    "    Input:  img     The image, either as a 3-D tensor (one image) or a\n",
    "                    4-D tensor (a stack of images).  In the latter case,\n",
    "                    the index argument specifies which image to display.\n",
    "            bboxes  The bounding boxes, as a N x 4 tensor, in normalized\n",
    "                    XYWH format.\n",
    "            cls     The class indices associated with the bounding boxes\n",
    "                    as a N x 1 tensor.\n",
    "            batch_idx   The index of each bounding box within the stack of\n",
    "                        images.  Ignored if img is 3-D.\n",
    "            index   The index of the image in the stack to be displayed.\n",
    "                    Ignored if img is 3-D.\n",
    "            **kw    All other keyword arguments are accepted and ignored.\n",
    "                    This allows you to use dictionary unpacking with the\n",
    "                    values produced by a YOLO DataLoader.\n",
    "    \"\"\"\n",
    "    if img.ndim == 3:\n",
    "        image = img[None, :]\n",
    "        index = 0\n",
    "        batch_idx = torch.zeros((len(cls),))\n",
    "    elif img.ndim == 4:\n",
    "        # Get around Black / Flake8 disagreement\n",
    "        indp1 = index + 1\n",
    "        image = img[index:indp1, :]\n",
    "\n",
    "    inds = batch_idx == index\n",
    "    res = ultralytics.utils.plotting.plot_images(\n",
    "        images=image,\n",
    "        batch_idx=batch_idx[inds] - index,\n",
    "        cls=cls[inds].flatten(),\n",
    "        bboxes=bboxes[inds],\n",
    "        names=CLASS_DICT,\n",
    "        threaded=False,\n",
    "        save=False,\n",
    "    )\n",
    "\n",
    "    return Image.fromarray(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.9:** Plot the image and bounding boxes for index 0 of this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_with_bboxes(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ... weird looking.  It's not what our original images look like, is it?\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "If it's not weird looking, try looking at another index.  Eventually you'll find something weird looking!\n",
    "</div>\n",
    "\n",
    "The file names from the batch are stored in the `im_file` key.  We can use that to look up the original image associated with this index and see what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.10:** Display the original image file for this index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two, we can see that the original image was distorted and combined with other images before being loaded into the YOLO model.  The YOLO model applies a number of augmentation steps by default.  (You can take a look at [all of the augmentation settings](https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters) in YOLO.)  This increases the diversity of training images, which should help the model generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation with Torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're training a YOLO model, it's generally best to use the built-in augmentation setting.  But in other cases, you may need to implement an augmentation system yourself.  Torchvision makes this easy by providing a number of augmentation transforms in its transforms version 2 (v2) module.\n",
    "\n",
    "To demonstrate this, we'll load a sample image.  The code below will get the file paths for `01.jpg` and its associated label file.  (It's written so that it works whether the image ended up in the training or validation split.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_base = pathlib.Path(\"data_yolo\")\n",
    "sample_fn = next((yolo_base / \"images\").glob(\"*/01.jpg\"))\n",
    "sample_labels = next((yolo_base / \"labels\").glob(\"*/01.txt\"))\n",
    "\n",
    "print(sample_fn)\n",
    "print(sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.11:** Load the image with PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = ...\n",
    "\n",
    "sample_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.12:** Convert the image to a tensor.  In the transforms version 2 module, this can be done with the confusingly-named `ToImage` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_torch = ...\n",
    "\n",
    "print(sample_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding boxes are stored in the label file.  Let's take a look a the first five lines to remember what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n5 $sample_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line represents a bounding box.  The first element is the class index.  This is followed by the _x_ and _y_ coordinates of the box center, the width, and the height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.13:** Load the bounding box data into a variable named `label_data`.  It should be a list of the bounding boxes.  Each bounding box will itself be a list of five strings in the same order they are in the file.  Don't worry about converting the strings to numbers yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into `label_data`\n",
    "\n",
    "label_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.14:** Create a tensor containing the class indices.  For compatibility with our plotting function it should be a $N\\times 1$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ...\n",
    "\n",
    "print(\"Tensor shape:\", classes.shape)\n",
    "print(\"First 5 elements:\\n\", classes[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.15:** Load the bounding box coordinates into a $N\\times 4$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = ...\n",
    "\n",
    "print(\"Tensor shape:\", bboxes.shape)\n",
    "print(\"First 5 elements:\\n\", bboxes[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these coordinates are normalized by the width or height, as appropriate.  This won't work with transformations like rotation, which need the same units used on each axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.16:** Convert the bounding box coordinates to pixel units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_width, sample_height = sample_image.size\n",
    "\n",
    "scale_factor = ...\n",
    "\n",
    "bboxes_pixels = bboxes * scale_factor\n",
    "\n",
    "print(\"Tensor shape:\", bboxes_pixels.shape)\n",
    "print(\"First 5 elements:\\n\", bboxes_pixels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the transformations to know how to transform the bounding boxes, they need to know that the coordinates represent the centers and dimensions of the boxes.  This is done by creating a special `BoundingBoxes` tensor.  This type has a `format` attribute.  By setting this to `\"CXCYWH\"`, we're telling it that the columns represent the Center *X* coordinate, the Center *Y* coordinate, the Width, and the Height.  This tensor also is given the size of the image, so it doesn't need to look that up for transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_tv = torchvision.tv_tensors.BoundingBoxes(\n",
    "    bboxes_pixels,\n",
    "    format=\"CXCYWH\",\n",
    "    # Yes, that's right.  Despite using width x height everywhere\n",
    "    # else, here we have to specify the image size as height x\n",
    "    # width.\n",
    "    canvas_size=(sample_height, sample_width),\n",
    ")\n",
    "\n",
    "print(\"Tensor type:\", type(bboxes_tv))\n",
    "print(\"First 5 elements:\\n\", bboxes_tv[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that we did all of those conversions correctly.  Do the bounding boxes line up with the correct objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_bboxes(sample_torch, bboxes, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, we'll introduce some transformations.  The first one will be a horizontal flip.  Many everyday objects have bilateral symmetry (or nearly so), so a flipped image will still have the same object classes in it.  This makes a horizontal flip a good data augmentation transformation.\n",
    "\n",
    "(In contrast, up/down symmetry is less common.  A vertical flip is generally not as useful, unless you need to recognize upside-down objects.)\n",
    "\n",
    "The transforms version 2 module has a `RandomHorizontalFlip` transformation.  This takes the probability of a flip as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.17:** Use the `RandomHorizontalFlip` transformation to flip the sample image.  Set `p=1` to ensure that the flip happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = ...\n",
    "\n",
    "plot_with_bboxes(flipped, bboxes_tv, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image has flipped, but the bounding boxes are still in their original locations.  Note that the bus is now in the bottom left of the image.  Its bounding box is still at the bottom right, and it now contains some asphalt, planters and trees.  If we fed this into a model, it would make the model worse, by confusing it as to what a bus looks like.\n",
    "\n",
    "So, we need to transform the bounding box coordinates consistent with the image transformation.  The Torchvision version 2 transformations can take multiple arguments. They perform the same transformation on all of the arguments, returning a transformed version of each.  They also understand how to correctly transform the `BoundingBoxes` tensors, depending on their type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.18:** Use `RandomHorizontalFlip` to flip both the sample image and its bounding boxes.  Check that they line up correctly now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped, flipped_bboxes = ...\n",
    "\n",
    "plot_with_bboxes(flipped, flipped_bboxes, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.19:** Apply the `RandomRotation` transformation.  This takes an argument of the maximum number of degrees to rotate the image.  Set it to 90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated, rotated_bboxes = ...\n",
    "\n",
    "plot_with_bboxes(rotated, rotated_bboxes, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple augmentation techniques can be chained together to produce even more diversity in the training images.  Within Torchvision, this can be accomplished by the `Compose` element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.20:** Create an augmentation pipeline that combines the `RandomHorizontalFlip` with the `RandomRotation`.  This time, set the probability of the flip to 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose(\n",
    "    [\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformed, transformed_bboxes = ...\n",
    "\n",
    "plot_with_bboxes(transformed, transformed_bboxes, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a large number of transformations that can be used for data augmentation.  Scroll through [the documentation](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended) to get a view of the range of possibilities.\n",
    "\n",
    "In addition to the transforms we've already used, note:\n",
    "- [`RandomResizedCrop`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomResizedCrop.html#torchvision.transforms.v2.RandomResizedCrop) will randomly crop the image down, and then it resizes the output to a set dimension.\n",
    "- [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ColorJitter.html#torchvision.transforms.v2.ColorJitter) can randomly adjust the brightness, contrast, saturation, and hue of the image, within specified ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5.21:** Create an augmentation pipeline that applies several of these transformations.  Choose reasonable values for the parameters.  Check that the bounding boxes are transformed correctly through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "There's no right answer.  A good choice of augmentations depends heavily on the problem you're trying to model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ...\n",
    "\n",
    "transformed, transformed_bboxes = ...\n",
    "plot_with_bboxes(transformed, transformed_bboxes, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the transformation several times to see the different types of images that result.  This greater diversity of training images will help models learn to generalize instead of memorizing during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **‚ú® Conclusion: üöÄ** \n",
    "\n",
    "You've now explored **data augmentation** in action for object detection tasks. By boosting the diversity of training images, data augmentation enhances a model's **generalization ability**, ultimately improving its performance on unseen data, which is the ultimate goal of any machine learning system üìà  \n",
    "\n",
    "\n",
    "\n",
    "#### üß† Key Takeaways:  \n",
    "- **üì• Data Loader from YOLO**: Learned how to access and utilize the data loader in a YOLO model.  \n",
    "- **üé® Default YOLO Augmentation**: Observed the augmentation transforms YOLO applies during training.  \n",
    "- **üõ†Ô∏è Torchvision Transforms**: Discovered how to use **Torchvision transforms** for customized data augmentation.  \n",
    "- **üîó Transform Images & Labels Together**: Understood the importance of transforming both images and bounding box labels simultaneously.  \n",
    "- **‚öôÔ∏è Combining Techniques**: Learned how to compose multiple augmentation techniques for robust data preparation.  \n",
    "\n",
    "\n",
    "\n",
    "#### üí° Pro Tips:  \n",
    "- For **pre-trained models** like YOLO, the default data augmentation settings are typically well-optimized: it's best to use them as-is initially.  \n",
    "- Only adjust these settings when performance issues or specific needs arise.  \n",
    "- For **custom models**, data augmentation is your responsibility. Implementing it thoughtfully can significantly improve model performance.\n",
    "\n",
    "By using data augmentation effectively, you're setting your machine learning system up for success in real-world applications. üåü "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
